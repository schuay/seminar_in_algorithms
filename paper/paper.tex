\documentclass[a4paper,10pt]{article}

\usepackage{acronym}
\usepackage[backend=bibtex]{biblatex}
\usepackage[usenames,dvipsnames]{color}
\usepackage{comment}
\usepackage[pdfborder={0 0 0}]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{listings}

\input{../common/acronyms}
\bibliography{../common/bibliography.bib}

\definecolor{Gray}{gray}{0.5}
\definecolor{OliveGreen}{cmyk}{0.64,0,0.95,0.40}

\lstset{
    language=C++,
    basicstyle=\ttfamily,
    keywordstyle=\color{OliveGreen},
    commentstyle=\color{Gray},
    captionpos=b,
    breaklines=true,
    breakatwhitespace=false,
    showspaces=false,
    showtabs=false,
    numbers=left,
}

\title{State of the Art in Concurrent Priority Queues \\
       Student Paper for Seminar in Algorithms 2013W \\
       Technical University of Vienna}
\author{Jakob Gruber, 0203440}

\begin{document}

\maketitle

\begin{comment}
Abstract formula
----------------

1) big picture problem or topic widely debated in your field.
2) gap in the literature on this topic.
3) your project filling the gap.
4) the specific material that you examine in the paper.
5) your original argument.
6) a strong concluding sentence.

Introduction formula
--------------------

A good paper introduction is fairly formulaic. If you follow a simple set of
rules, you can write a very good introduction. The following outline can be
varied. For example, you can use two paragraphs instead of one, or you can
place more emphasis on one aspect of the intro than another. But in all cases,
all of the points below need to be covered in an introduction, and in most
papers, you don't need to cover anything more in an introduction.

Paragraph 1: Motivation. At a high level, what is the problem area you are
working in and why is it important? It is important to set the larger context
here. Why is the problem of interest and importance to the larger community?

Paragraph 2: What is the specific problem considered in this paper? This
paragraph narrows down the topic area of the paper. In the first paragraph you
have established general context and importance. Here you establish specific
context and background.

Paragraph 3: "In this paper, we show that ...". This is the key paragraph in
the intro - you summarize, in one paragraph, what are the main contributions of
your paper given the context you have established in paragraphs 1 and 2. What
is the general approach taken? Why are the specific results significant? This
paragraph must be really really good. If you can't "sell" your work at a high
level in a paragraph in the intro, then you are in trouble. As a reader or
reviewer, this is the paragraph that I always look for, and read very
carefully.

You should think about how to structure this one or two paragraph summary of
what your paper is all about. If there are two or three main results, then you
might consider itemizing them with bullets or in test (e.g., "First, ..."). If
the results fall broadly into two categories, you can bring out that
distinction here. For example, "Our results are both theoretical and applied in
nature. (two sentences follow, one each on theory and application)"

Paragraph 4: At a high level what are the differences in what you are doing,
and what others have done? Keep this at a high level, you can refer to a future
section where specific details and differences will be given. But it is
important for the reader to know at a high level, what is new about this work
compared to other work in the area.

Paragraph 5: "The remainder of this paper is structured as follows..." Give the
reader a roadmap for the rest of the paper. Avoid redundant phrasing, "In
Section 2, In section 3, ... In Section 4, ... " etc.

A few general tips:

Don't spend a lot of time into the introduction telling the reader about what
you don't do in the paper. Be clear about what you do do, but don't dwell here
on what you don't do.  Does each paragraph have a theme sentence that sets the
stage for the entire paragraph? Are the sentences and topics in the paragraph
all related to each other?  Do all of your tenses match up in a paragraph?
\end{comment}

\begin{abstract}
On the 24th of February, 1815, the watch-tower of Notre-Dame de la Garde
signalled the arrival of the three-master Pharaon, from Smyrna, Trieste, 
and Naples.

The usual crowd of curious spectators immediately filled the quay of Fort 
Saint-Jean, for at Marseilles the arrival of a ship is always a great event, 
especially when that ship, as was the case with the Pharaon, has been built, 
rigged, and laden in the dockyard of old Phocaea and belongs to a shipowner 
of their own town.

Meanwhile the vessel drew on, and was approaching the harbour under topsails, 
jib, and foresail, but so slowly and with such an air of melancholy that the 
spectators, always ready to sense misfortune, began to ask one another what 
ill-luck had overtaken those on board. However, those experienced in navigation 
soon saw that if there had been any ill-luck, the ship had not been the 
sufferer, for she advanced in perfect condition and under skilful handling; 
the anchor was ready to be dropped, the bowsprit shrouds loose. 
Beside the pilot, who was steering the Pharaon through the narrow entrance to 
the port, there stood a young man, quick of gesture and keen of eye, who 
watched every movement of the ship while repeating each of the pilot's orders.
\end{abstract}

\section{Introduction}

% Move to many cores, why do we need concurrent algorithms/data structures?

In the past decade, advancements in computer performance have been made mostly
through an increasing number of processors instead of higher clock speeds.
This development necessitates new approaches to data structures and algorithms
that take advantage of concurrent execution on multiple threads and processors. 

% The priority queue - semantics and usage summary.

This paper focuses on the priority queue data structure, consisting of two operations
traditionally called \lstinline|Insert| and \lstinline|DeleteMin|. \lstinline|Insert|
places an item into the queue together with its priority, while \lstinline|DeleteMin|
removes and returns the highest priority item. Both of these operations are expected to have
a complexity of at most $O(\log n)$. Priority queues are used in a large variety
of situations such as shortest path algorithms and scheduling. % See liu2012lock for more.

% Concurrent priority queues - heaps, trees, skiplists.

Concurrent priority queues have been the subject of research since the 1980s.
While early efforts have focused mostly on parallelizing Heap structures
\cite{hunt1996efficient}, % TODO: Many more, see \cite{shavit2000skiplist}
more recently priority queues based on \citeauthor{pugh1990skip}'s SkipLists
\cite{pugh1990skip} seem to show more potential \cite{shavit2000skiplist,
sundell2003fast,herlihy2012art,linden2013skiplist}.

% Our plan for the paper - somehow this kind of duplicates the last paragraph though.

In the following, we examine the evolution of concurrent priority queues. We begin
with first steps which used fine-grained locking to avoid the bottleneck of a single
global lock. Initial SkipList-based structures offered better disjoint-access
parallelism, and finally current state of the art lock-free priority queues minimize
contention in \lstinline|DeleteMin|.

% TODO: Mention what we don't cover: non-general PQs, unavailable primitives, ...
% TODO: In section x, we ... Again, this partially duplicates the last paragraph.

\section{Concepts and Definitions}

% Blocking, lock-free and wait-free data structures.

Concurrent data structures are intended to be accessed simultaneously by several processes
at once. \emph{Lock-based} structures ensure that only a limited number of processes may enter
a critical section at once. \emph{Lock-free} data structures eschew the use of locks, and guarantee
that at least a single process makes progress at all times. Since lock-free structures are
non-blocking, they are not susceptible to priority inversion, deadlock, and livelock.
\emph{Wait-freedom} further guarantees that every process finishes each operation in a bounded number of steps. 
In practise, wait-freedom often introduces an unacceptable overhead; lock-freedom
however has proven to be both efficient and to scale well to large numbers of processes.

% Linearizability, sequential and quiescent consistency.

There are several different criteria which allows reasoning about the correctness of concurrent
data structures. \emph{Linearizable} \cite{herlihy1990linearizability} operations appear to take
effect at a single instant in time at so-called linearization points. Linearizability is composable ---
any data structure composed of linearizable objects is also linearizable.
\emph{Sequential consistency} \cite{lamport1979make} requires the result of a set of operations
executed in parallel to be equivalent to the result of some sequential ordering of the same
operations.
\emph{Quiescently consistent} \cite{shavit1996diffracting} data structures
guarantee that the result of a set of parallel operations is equal to the result of a sequential ordering
after a period of quiescence, i.e. an interval without active operations has passed.

% Disjoint-access parallelism.

An area in memory accessed frequently by a large number of processes is said to be \emph{contended}.
Contention is an limiting factor regarding scalability: concurrent reads and writes to the same 
location must be serialized by the cache coherence protocol, and only a single concurrent \ac{CAS} % TODO: Short section on primitives.
can succeed while all others must retry. \emph{Disjoint-access parallelism} is the concept of
spreading such accesses in order to reduce contention as much as possible. 

% Priority queues.

Priority queues are an abstract data structure allowing insertion of items with a given priority
and removal of the highest-priority item in logarithmic time. Search trees and Heaps (which are 
flattened representations of complete trees such that each node's value is at least as large as
those of both children) are concrete data structures which are usually used to implement sequential
priority queues. However, both require fairly elaborate reorganization after \lstinline|DeleteMin|
and/or \lstinline|Insert| operations, which are especially challenging to achieve in a concurrent
environment. 

% SkipLists.

SkipLists \cite{pugh1990skip} have become increasingly popular in concurrent data structures because 
they are both simple and exhibit excellent disjoint-access parallelism. In contrast to
search trees and Heaps, reorganization is not necessary, since SkipLists rely on
randomization for an expected $O(\log n)$ complexity of \lstinline|Insert|, \lstinline|Search|
and \lstinline|Delete| operations.
A SkipList may be visualized as a set of linked lists with corresponding levels. The linked
list at level 0 is the sorted sequence of all objects in the SkipList,
and higher levels provide ``shortcuts''
into the SkipList such that a list of level $i + 1$ contains a subset of the objects in level
$i$. A SkipList node $n$ is said to be of level $i$ if it is in all lists of levels $[0, i]$
and in none of levels $[i + 1, \infty]$. Upon insertion, the new node's level is assigned at random
according to a geometric distribution; deletion simply removes the node.

% TODO: Maybe integrate into description - otherwise remove. Do we need further detail?

\begin{figure}[h]
\begin{lstlisting}
struct slist_t {
  size_t max_level;
  node_t head[max_level];
};

struct node_t {
  key_t key;
  value_t value;
  size_t level;
  node_t *next[level];
};

/* All operations are expected O(log n) time. */
void slist_insert(slist_t *l, key_t k, value_t v);
bool slist_delete(slist_t *l, key_t k, value_t &v);
\end{lstlisting}
\caption{Basic SkipList structure and operations.}
\label{fig:basicsl}
\end{figure}

\section{Fine-grained Locking Heaps} \label{sec:hunt}

We chose the priority queue by \citeauthor{hunt1996efficient} \cite{hunt1996efficient}
as a representative of early concurrent priority queues since it has been proven to
perform well \cite{shavit2000skiplist} in comparison to other efforts of the time such as \cite{nageshwara1988concurrent,ayani1990lr,yan1998lock}. % Many others \cite{shavit2000skiplist}.
It is based on a Heap
structure and attempts to minimize lock contention between threads by a) adding per-node
locks, b) spreading subsequent insertions through a bit-reversal technique, % Elaborate on these.
and c) letting insertions traverse bottom-up in order to minimize conflicts with
top-down deletions.

However, significant limitations to scalability remain. A global lock is required
to protect accesses to a variable storing the Heap's size which all operations
must obtain for a short time. Disjoint-access through bit-reversal breaks down
once a certain amount of traffic is reached, since only subsequent insertions
are guaranteed to take disjoint paths towards the root node. Note also that
the root node is a severe serial bottleneck, since it is potentially part of
every insertion path, and necessarily of every \lstinline|DeleteMin| operation.
Finally, in contrast to later SkipList-based designs, the capacity of Hunt Heap
is fixed upon creation.

Benchmarking results in the literature have been mixed; a sequential priority
queue protected by a single global lock outperforms the \citeauthor{hunt1996efficient}
Heap in most cases \cite{hunt1996efficient,sundell2003fast}. Speed-up only occurs once
the size of the Heap reaches a certain threshold such that concurrency
can be properly exploited (instead of being dominated by global locking overhead).

\section{Lock-free Priority Queues} \label{sec:lockfree}

Traditional data structures such as the Heap have fallen out of favor;
instead, SkipLists \cite{pugh1990skip,pugh1998concurrent} have become the focus
of modern concurrent priority queue research
\cite{shavit2000skiplist,sundell2003fast,herlihy2012art,linden2013skiplist,alistarhspraylist}.
SkipLists are both conceptual simple as well as simple to implement; they also exhibit
excellent disjoint-access properties, and do not require rebalancing due to their
reliance on randomization for expected $O(\log n)$ time insertion and deletion bounds.

A state of the art lock-free SkipList implementation based on \ac{CAS}
by \citeauthor{fraser2004practical} \cite{fraser2004practical} is freely available\footnote{
\url{http://www.cl.cam.ac.uk/research/srg/netos/lock-free/}} under a BSD license.
\citeauthor{fraser2004practical} exploits unused pointer bits to mark nodes as logically
deleted, with physical deletion following as a second step.

SkipLists are dynamic data structures in the sense that they grow and shrink
at runtime. In consequence, careful handling of memory accesses and (de)allocations
are required. As an additional requirement, these memory management schemes must
be both scalable and lock-free to avoid limiting the SkipList itself.
\citeauthor{fraser2004practical} in particular employs lock-free epoch-based garbage-collection.

\subsection{\citeauthor{shavit2000skiplist}} \label{sec:shavit}

\citeauthor{shavit2000skiplist} were the first to propose the use of SkipLists
for priority queues \cite{linden2013skiplist}. Their initial implementation
\cite{shavit2000skiplist} builds on \citeauthor{pugh1998concurrent}'s concurrent
SkipList \cite{pugh1998concurrent} which uses one lock per node per level.

% TODO: This counts for all skiplist impls.

A crucial observation is that nodes nodes which are only partially connected
do not affect correctness of the data structure. As soon as the first level (i.e. \lstinline|node.level[0]|)
has been successfully connected, a node is considered to be in the SkipList.
Therefore, both insertions and deletions can be split into steps --- insertions
proceed bottom-up while deletions proceed top-down, holding locks only for the current level.

\begin{figure}[h]
\begin{lstlisting}
struct node_t {
  [...] /**< Standard node members as above. */
  atomic<bool> deleted; /**< Initially false. */
  time_t timestamp;
  lock_t locks[level + 1];
};
\end{lstlisting}
\caption{\citeauthor{shavit2000skiplist} structure.}
\label{fig:shavitsl}
\end{figure}

Likewise, deletions are split into a logical phase, atomically setting the \lstinline|node.deleted|
flag; and a physical phase performs the actual pointer manipulations and can be seen as a simple
call to the underlying SkipList's \lstinline|sl_delete| function.

A \lstinline|DeleteMin| call starts the list head, and attempts to atomically set
the deletion flag using a \lstinline|CAS(node.deleted, false, true)| call (or equivalent constructs).
If it succeeds, the current node is physically deleted and returned to the caller. Otherwise,
\lstinline|node.next[0]| is set as the new current node and the procedure is repeated.
If the end of the list is reached, \lstinline|DeleteMin| returns false to indicate an empty list.

% Logical/physical deletion separation allows concurrent physical deletions.

Note that so far, this implementation is not linearizable: consider the case in which a
slow thread A is in the middle of a \lstinline|DeleteMin| call. Within this context, we refer to 
the node with key $i$ as ``node $i$'', or simply $i$. Several \lstinline|CAS|
operations have failed, and A is currently at node $j$.
A fast thread B then first inserts a node $i$, followed by a node $k$ such that
$i < j < k$, i.e. the former and latter nodes are inserted, respectively, before and after
thread A's current node. Assuming further that all nodes between $j$ and $k$ have already
been deleted, then thread A will return node $k$. This execution is not linearizable; however,
it is quiescently consistent since operations can be reordered to correspond to some sequential
execution at quiescent periods.

\citeauthor{shavit2000skiplist} counteract this by introducing a \lstinline|timestamp| for each
node which is set upon successful insertion. Each \lstinline|DeleteMin| operation 
simply ignores all nodes it sees that have not been fully inserted at the time it was called.

% TODO: Linearizability points.

Explicit memory management is required to avoid dereferencing pointers to freed memory areas
by other threads after physical deletion. This implementation uses a dedicated garbage collector
thread in combination with a timestamping mechanism which frees \lstinline|node|'s memory only
when all threads that might have seen a pointer to \lstinline|node| have exited the data structure.

\citeauthor{herlihy2012art} \cite{herlihy2012art} later described and implemented a variant of this idea 
in Java that is lock-free and quiescently consistent. While mostly identical, notable differences are
that a) the new variant is based on a lock-free skiplist, b) the timestamping mechanism was not
employed and thus linearizability was lost, and c) explicit memory management is not required
because the Java virtual machine provides a garbage collector.

\subsection{\citeauthor{sundell2003fast}} \label{sec:sundell}

\citeauthor{sundell2003fast} proposed the first lock-free concurrent priority queue in
\citeyear{sundell2003fast} \cite{sundell2003fast}. The data structure is linearizable
and is implemented using commonly available atomic primitives \ac{CAS}, \ac{TAS}, and \ac{FAA}.
In contrast to other structures covered in this paper, this priority queue is restricted to
contain items with pairwise distinct priorities.
A real-time version is also provided which will we will not discuss further (interested readers are
referred to \cite{sundell2003fast}).

\begin{figure}[h]
\begin{lstlisting}
struct node_t {
  [...] /**< Standard node members as above. */
  size_t valid_level;
  node_t *prev;
};
\end{lstlisting}
\caption{\citeauthor{sundell2003fast} structure.}
\label{fig:sundellsl}
\end{figure}

The structure of each node is basically identical to Figure \ref{fig:basicsl}. However, \citeauthor{sundell2003fast}
exploit the fact that the two least significant bits of pointers on 32- and 64-bit systems
are unused and reuse pointers as deletion marks. A set least significant bit signifies on a pointer signifies
that the current node is about to be deleted. We will use the notation 
\lstinline|CAS(node.level[i], (ptr, false), (ptr, true))| to signify \ac{CAS} operations on the
combined pointer \& flag fields.
Reuse of \lstinline|node.level[i]| pointers
prevents situations in which a new node is inserted while its predecessor is being removed,
effectively deleting both nodes from the list. Likewise, the reuse of the \lstinline|node.value|
pointer ensures that updates of pointer values (which occur when a node with the inserted priority already exists)
handle concurrent node removals correctly.

As in the \citeauthor{shavit2000skiplist} priority queue, insertions proceed bottom-up while
deletions proceed top-down --- on the one hand, the choice of opposite directions reduces collisions
between concurrent insert and delete operations, while on the other hand removing nodes from top levels first
allows many other concurrent operations to simply skip these nodes, further improving performance.
\lstinline|node.valid_level| is updated during inserts to always equal the highest level of the SkipList
at which pointers in this node have already been set (as opposed to \lstinline|node.level|, which equals
the final level of the node).

A helping mechanism is employed whenever a node is encountered that has its deletion bit set, which attempts
to set the deletion bits on all next pointers and then removes the node from the current level. The
\lstinline|node.prev| pointer is used as a shortcut to the previous node, avoiding a complete retraversal
of the list.

% TODO: Mention backoff.

This implementation uses the lock-free memory management by invented by \citeauthor{valois1996lock}
\cite{valois1995lock,valois1996lock} and corrected by \citeauthor{michael1995correction} 
\cite{michael1995correction}. It was chosen in particular because this scheme can guarantee validity
of \lstinline|prev| as well as all \lstinline|next| pointers. Additionally, it does not require a separate
garbage collector thread.

A rigorous linearizability proof is provided in the original paper \cite{sundell2003fast} which shows
linearizability points for all possible outcomes of both \lstinline|Insert| and \lstinline|DeleteMin|
operations.

% TODO: Further details?

Benchmarks performed by \citeauthor{sundell2003fast} show their queue performing noticeably better than both locking 
queues from Sections \ref{sec:shavit} and \ref{sec:hunt}, and slightly better than a priority queue
consisting of a SkipList protected by a single global lock.

\subsection{\citeauthor{linden2013skiplist}} \label{sec:linden}

One of the most recent priority queue implementations was published by \citeauthor{linden2013skiplist}
in \citeyear{linden2013skiplist} \cite{linden2013skiplist}. They present a linearizable, lock-free concurrent priority
queue which achieves a speed-up of $30-80\%$ compared to other SkipList-based concurrent priority queues by
minimizing the number of \ac{CAS} operations within most \lstinline|DeleteMin| operations.

A priority queue implementation is called deterministic when the algorithm does not contain randomized elements.
It is called strict when \lstinline|DeleteMin| is guaranteed to return the minimal element currently within the queue
(as opposed to relaxed data structures which are discussed further in the next section).
All such priority queues share an inherent bottleneck, since all threads calling \lstinline|DeleteMin| compete
for the minimal element, causing both contention through concurrent \ac{CAS} operations on the same variable
as well as serialization effort by the cache coherence protocol for all other processor accessing the same cache
line.

\begin{figure}[h]
\begin{lstlisting}
struct node_t {
  [...] /**< Standard node members as above. */
  atomic<bool> inserting;
};
\end{lstlisting}
\caption{\citeauthor{linden2013skiplist} structure.}
\label{fig:lindensl}
\end{figure}

% TODO: Mention inserting flag and memory management!

In this implementation, \lstinline|DeleteMin| operations only perform logical deletion by setting
the deletion flag with a single \ac{FAO} call; nodes are only deleted physically once a certain
threshold of logically deleted nodes is reached.

This mechanism requires a new invariant, in that the set of all logically deleted nodes must always
form a prefix of the SkipList. Recall that in the \citeauthor{sundell2003fast} queue, deletion flags
for node \lstinline|n| were packed into \lstinline|n.next| pointers, preventing insertion of new
nodes \emph{after} deleted nodes. This implementation instead places the deletion flag into the
lowest level \lstinline|next| pointer of the previous node, preventing inserations \emph{before}
logically deleted nodes.

Once the prefix of logically deleted nodes reaches a specified length {\lstset{breaklines,breakatwhitespace} \lstinline|BoundOffset|},
the first thread to observe this fact within \lstinline|DeleteMin| performs the actual physical
deletions by updating \lstinline|slist.head[0]| to point at the last logically deleted node with a
single \ac{FAO} operation. The remaining \lstinline|slist.head| pointers are then updated, and
all physically deleted nodes are marked for recycling.

Since at any time, the data structure contains a prefix of logically deleted nodes, all \lstinline|DeleteMin|
operations must traverse this sequence before reaching a non-deleted node. In general, reads of nonmodified
memory locations are very cheap; however, benchmarks in \cite{linden2013skiplist} have shown that
after a certain point, the effort spent in long read sequences significantly outweighs the reduced
number of \ac{CAS} calls. It is therefore crucial to choose \lstinline|BoundOffset| carefully, with the
authors recommending a prefix length bound of 128 for 32 threads.

The actual \lstinline|DeleteMin| and \lstinline|Insert| implementations are surprisingly simple.
Deletions simply traverse the list until the first node for which \lstinline|(ptr, d) = FAO((node.next[0], d), 1)|
returns a previously unset deletion flag (\lstinline|d = 0|) and then return \lstinline|ptr|.
Insertions occur bottom-up and follow the basic \citeauthor{fraser2004practical} algorithm \cite{fraser2004practical},
taking the separation of deletion flags and nodes into account.

The authors provide high level correctness and linearizability proofs as well as a model for the
SPIN model checker. Performance has been shown to compare favorably to both
\citeauthor{sundell2003fast} and \citeauthor{shavit2000skiplist} queues, with throughput improved by
up to $80\%$.

\section{Relaxed Priority Queues}

The body of work discussed in previous sections
creates the impression that the outer limits of strict, deterministic priority queues have been reached.
In particular, \citeauthor{linden2013skiplist} conclude that scalability is solely limited by \lstinline|DeleteMin|,
and that less than one modified memory location per thread and operation would have to be read
in order to achieve improved performance \cite{linden2013skiplist}.

Recently, relaxation of provided guarantees have been investigated as another method of reducing
contention and improving disjoint-access parallelism.
For instance, k-FIFO queues \cite{kirsch2012fast} have achieved considerable
speed-ups compared to strict FIFO queues by allowing {\lstset{breaklines,breakatwhitespace} \lstinline|Dequeue|} to return elements
up to $k$ positions out of order.

Relaxation has also been applied to concurrent priority queues with some success, and in the following
sections we discuss two such approaches.

\subsection{\citeauthor{wimmer2013data}} \label{sec:wimmer}

\citeauthor{wimmer2013data} presented the first relaxed, linearizable, and lock-free priority queue
in \cite{wimmer2013data}. It is integrated as a priority scheduler into their \emph{Pheet} task-scheduling
system, and an open-source implementation is available\footnote{\url{http://pheet.org}}.

Their paper presents several variations on the common theme of priority queues: a distributed work-stealing
queue which can give no guarantees as to global ordering since it consists of separate priority queues
at each thread; a relaxed centralized priority queue in which no more than $k$ items are missed
by any processor; and a relaxed hybrid data structure which combines both ideas and provides
a guarantee that no thread misses more than $kP$ items where $P$ is the number of participating threads.
In this section, we examine only the hybrid variant since it provides both the scalability of work-stealing
queues and the ordering guarantees of the centralized priority queue.

\begin{figure}[h]
\begin{lstlisting}
struct globals_t {
  list_of_item_t global_list;
};

/* Thread-local items. */
struct locals_t {
  list_of_item_t local_list;
  pq_t prio_queue;
  size_t remaining_k;
};
\end{lstlisting}
\caption{\citeauthor{wimmer2013data} structure.}
\label{fig:wimmerq}
\end{figure}

The hybrid queue consists of a list of globally visible items and one local item list as well as a local
sequential priority queue per thread. The thread-local counter \lstinline|remaining_k| tracks how many more
items may be added to the local queue until all local items must be made globally visible to avoid
breaking guarantees.

Whenever an item is added, it is first added to both \lstinline|local_list| and \lstinline|prio_queue|
and \lstinline|remaining_k| is decremented. If \lstinline|remaining_k| reaches zero, then the local
item list is appended to the global list, and all not yet seen (by this thread) items of the global
queue are added to the local priority queue.

\lstinline|DeleteMin| simply pops the local priority queue as long is it is non-empty and the popped
item has already been deleted. When a non-deleted item is popped, it is atomically marked as deleted
and returned to the caller. If instead we are faced with an empty local queue, we attempt to spy,
i.e. copy items from another thread's local list.

The \citeauthor{wimmer2013data} priority queue was evaluated using a label-correcting variant
of Dijkstra's shortest path algorithms. Their model creates a task for each node
expansion, and therefore comes with a considerable task scheduling overhead. Nonetheless,
the parallel implementation scales well up to 10 threads, and further limited performance
gains are made until 40 threads. To date, no direct comparisons to other concurrent
priority queues have been possible since the data structure is strongly tied to \emph{Pheet},
but a separate implementation of k-priority queues is planned.

\subsection{SprayList} \label{sec:spraylist}

The SprayList is another recent approach towards a relaxed priority queue by \citeauthor{alistarhspraylist}
\cite{alistarhspraylist}. Instead of the distributed approach described in the previous section,
the SprayList is based on a central data structure, and uses a random walk in \lstinline|DeleteMin|
in order to spread accesses over the $O(P \log^3 P)$ first elements with high probability, where $P$
is again the number of participating threads.

\citeauthor{fraser2004practical}'s lock-free SkipList \cite{fraser2004practical} again serves as the
basis for the priority queue implementation. The full source code is available online\footnote{
\url{https://github.com/jkopinsky/SprayList}}. In the SprayList, \lstinline|Insert| calls are simply
forwarded to the underlying SkipList.

The \lstinline|DeleteMin| operation however executes a random walk, also called a \emph{spray}, the
purpose of which is to spread accesses over a certain section of the SkipList uniformly such that
collisions between multiple concurrent \lstinline|DeleteMin| calls are unlikely. This is achieved by
starting at some initial height, walking a randomized number of steps, descending a randomized number of levels,
and repeating this procedure until a node $n$ is reached on the lowest level. If $n$ is not deleted,
it is logically deleted and returned. Otherwise, a \emph{spray} is either reattempted, or the thread
becomes a cleaner, traversing the lowest level of the SkipList and physically removing logically deleted
nodes it comes across. A number of dummy nodes are added to the beginning of the list in order to counteract
the algorithm's bias against initial items.

The \emph{spray} parameters are chosen such that with high probability, one of the $O(P \log^3 P)$
first elements is returned, and that each of these elements is chosen roughly uniformly at random.
The final effect is that accesses to the data structure are spread out, reducing contention and resulting
in a noticeably lower number of \ac{CAS} failures in comparison to strict priority queues described
in Section \ref{sec:lockfree}.

The authors do not provide any statement as to the linearizability (or other concurrent correctness
criteria) of the SprayList, and it is not completely clear how to define it since no
sequential semantics are given.

Benchmarks show promising results: the SprayList scales well at least up to 80 threads,
and performs close (within a constant factor) to an implementation using a random remove instead
of \lstinline|DeleteMin|, which the authors consider as the performance ideal.

% Maybe mention that P log^3 P is a very loose bound.

\section{Performance Results}

In this section, we compare the performance of several different concurrent priority
queue implementations:

\begin{itemize}
\item \textbf{GlobalLock} An instance of the \lstinline|std::priority_queue<T>| class provided
      by the C++ standard library, protected by a single global lock.
\item \textbf{Hunt} \citeauthor{hunt1996efficient} provide an implementation
      of their Heap-based design \cite{hunt1996efficient} at \url{ftp://ftp.cs.rochester.edu/pub/packages/concurrent_heap/}.
      However, as the original code was written for the MIPS architecture, we chose to
      use the alternative implementation in \emph{libcds}\footnote{\url{http://sourceforge.net/p/libcds/code/}}
      instead.
\item \textbf{Noble} An implementation of the \citeauthor{sundell2003fast} priority queue \cite{sundell2003fast}
      is available in the Noble library\footnote{\url{http://www.non-blocking.com/Eng/download-demo.aspx}}.
      \textbf{Noble} is lock-free, based on SkipLists, and limited to unique priorities.
      According to the authors, the commercial Noble library includes a more efficient
      version of this data structure that can also handle duplicate priority values.
\item \textbf{Linden} Code for the \citeauthor{linden2013skiplist} priority queue \cite{linden2013skiplist}
      is provided by the authors under an open source license\footnote{\url{http://user.it.uu.se/~jonli208/priorityqueue}}.
      It is lock-free, SkipList-based, and uses \citeauthor{fraser2004practical}'s lock-free
      SkipList implementation. The aim of this implementation is to minimize contention in
      calls to \lstinline|DeleteMin|.
\item \textbf{SprayList} A relaxed concurrent priority queue based on \citeauthor{fraser2004practical}'s
      SkipList using random walks to spread data accesses
      incurred by \lstinline|DeleteMin| calls. Code provided by \citeauthor{alistarhspraylist} is
      available on Github\footnote{\url{https://github.com/jkopinsky/SprayList}}.
\end{itemize}

In each test run, the examined priority queue was initially filled with $2^{15}$
elements. We then ran a tight loop of $50\%$ insertions and $50\%$ deletions
for a total of $10$ seconds, where all \lstinline|Insert|
operations within this context implicitly choose a key uniformly at random from
the range of all 32-bit integers. This methodology seems to be the de facto standard for concurrent % TODO: Similar phrasing as linden :(
priority queue benchmarks
\cite{alistarhspraylist,linden2013skiplist,shavit2000skiplist,sundell2003fast}.
Each run was repeated for a total of $10$ times
and we report on the average throughput.

All benchmarks were compiled with \verb|-O3| using GCC 4.8.2. Evaluations took place
on an 80-core Intel Xeon machine. The benchmarking code was adapted from \citeauthor{linden2013skiplist}'s
benchmarking suite and is available at \url{https://github.com/schuay/seminar_in_algorithms}.

% TODO: Figures, interpretation. Mention that Noble requires pointers -> new/delete.
% Better hardware description - see SprayList section 4, Linden p16.
% Note: Strong degradation of noble PQ with growing initial size.
% Standard PQ benchmarking methodology, cite linden + ??.
% Drop-off when using more than 1 socket, Linden p17.

\begin{comment}
Sections / rough structure:
* Basic concepts and definitions. Linearizability, sequ./quiescent consistency,
  lock-free, wait-free, disjoint-access parallelism (one of the papers has good
  summaries of these).
  Maybe atomic primitives such as CAS (but probably not).
* Priority queue definitions, semantics, usages. Mention inherent
  non-scalability through DeleteMin().
* Skiplist, heap definitions, semantics, usages.
* Follow development of current state of the art from Hunt Heap ~> Shavit ~>
  Tsigas ~> Linden.
* Maybe benchmarks on mars.

Additional notes:
* Sketch development: Heap, Fraser Skiplist (Details, mention why linearizable),
  Sundell/Tsigas, other Skiplist-based PQs
* Modern developments: SprayList, Wimmer et al. Relation to other relaxed structures such as k-FIFO.
\end{comment}

\section{Related Work}

% Bounded priority queues
% PQ's with non-available primitives
% Other relaxed data structures -> k-FIFO
% ... ?

\begin{comment}
  * hunt: most efficient old-school algorithm
  * lots of other older references in [4]
  * israeli, rappoport: wait-free, non-available atomic primitive
  * lotan, shavit [4]: skiplist, lock-based. first to propose usage of skip-lists [11]
  * sundell, tsigas [3]: skiplist, lock-free, linearizable, unique priorities
  * herlihy, shavit in art of multiprocessor programming: based on [4], lock-free.
  * linden, jonsson [11]: skiplist, lock-free, linearizable, reduced mem contention.
    first lock-free PQ algorithm [11, 12]
  * specialized versions such as
    * bounded priorities: [7] and others
    * probabilistic extractMin, extractMany: [9]
\end{comment}

\section{Conclusion}

\begin{comment}
http://leo.stcloudstate.edu/acadwrite/conclude.html
http://writingcenter.unc.edu/handouts/conclusions/
http://www.wikihow.com/Write-a-Conclusion-for-a-Research-Paper
\end{comment}

\printbibliography

\end{document}
