\documentclass[a4paper,10pt]{article}

\usepackage{acronym}
\usepackage[backend=bibtex]{biblatex}
\usepackage[usenames,dvipsnames]{color}
\usepackage{comment}
\usepackage[pdfborder={0 0 0}]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{listings}

\input{../common/acronyms}
\bibliography{../common/bibliography.bib}

\definecolor{Gray}{gray}{0.5}
\definecolor{OliveGreen}{cmyk}{0.64,0,0.95,0.40}

\lstset{
    language=C++,
    basicstyle=\ttfamily,
    keywordstyle=\color{OliveGreen},
    commentstyle=\color{Gray},
    captionpos=b,
    breaklines=true,
    breakatwhitespace=false,
    showspaces=false,
    showtabs=false,
    numbers=left,
}

\title{Lock-free Skiplist-based Priority Queues \\
       Student Paper for Seminar in Algorithms 2013W \\
       Technical University of Vienna}
\author{Jakob Gruber, 0203440}

\begin{document}

\maketitle

\begin{comment}
Abstract formula
----------------

1) big picture problem or topic widely debated in your field.
2) gap in the literature on this topic.
3) your project filling the gap.
4) the specific material that you examine in the paper.
5) your original argument.
6) a strong concluding sentence.

Introduction formula
--------------------

A good paper introduction is fairly formulaic. If you follow a simple set of
rules, you can write a very good introduction. The following outline can be
varied. For example, you can use two paragraphs instead of one, or you can
place more emphasis on one aspect of the intro than another. But in all cases,
all of the points below need to be covered in an introduction, and in most
papers, you don't need to cover anything more in an introduction.

Paragraph 1: Motivation. At a high level, what is the problem area you are
working in and why is it important? It is important to set the larger context
here. Why is the problem of interest and importance to the larger community?

Paragraph 2: What is the specific problem considered in this paper? This
paragraph narrows down the topic area of the paper. In the first paragraph you
have established general context and importance. Here you establish specific
context and background.

Paragraph 3: "In this paper, we show that ...". This is the key paragraph in
the intro - you summarize, in one paragraph, what are the main contributions of
your paper given the context you have established in paragraphs 1 and 2. What
is the general approach taken? Why are the specific results significant? This
paragraph must be really really good. If you can't "sell" your work at a high
level in a paragraph in the intro, then you are in trouble. As a reader or
reviewer, this is the paragraph that I always look for, and read very
carefully.

You should think about how to structure this one or two paragraph summary of
what your paper is all about. If there are two or three main results, then you
might consider itemizing them with bullets or in test (e.g., "First, ..."). If
the results fall broadly into two categories, you can bring out that
distinction here. For example, "Our results are both theoretical and applied in
nature. (two sentences follow, one each on theory and application)"

Paragraph 4: At a high level what are the differences in what you are doing,
and what others have done? Keep this at a high level, you can refer to a future
section where specific details and differences will be given. But it is
important for the reader to know at a high level, what is new about this work
compared to other work in the area.

Paragraph 5: "The remainder of this paper is structured as follows..." Give the
reader a roadmap for the rest of the paper. Avoid redundant phrasing, "In
Section 2, In section 3, ... In Section 4, ... " etc.

A few general tips:

Don't spend a lot of time into the introduction telling the reader about what
you don't do in the paper. Be clear about what you do do, but don't dwell here
on what you don't do.  Does each paragraph have a theme sentence that sets the
stage for the entire paragraph? Are the sentences and topics in the paragraph
all related to each other?  Do all of your tenses match up in a paragraph?
\end{comment}

\begin{abstract}
On the 24th of February, 1815, the watch-tower of Notre-Dame de la Garde
signalled the arrival of the three-master Pharaon, from Smyrna, Trieste, 
and Naples.

The usual crowd of curious spectators immediately filled the quay of Fort 
Saint-Jean, for at Marseilles the arrival of a ship is always a great event, 
especially when that ship, as was the case with the Pharaon, has been built, 
rigged, and laden in the dockyard of old Phocaea and belongs to a shipowner 
of their own town.

Meanwhile the vessel drew on, and was approaching the harbour under topsails, 
jib, and foresail, but so slowly and with such an air of melancholy that the 
spectators, always ready to sense misfortune, began to ask one another what 
ill-luck had overtaken those on board. However, those experienced in navigation 
soon saw that if there had been any ill-luck, the ship had not been the 
sufferer, for she advanced in perfect condition and under skilful handling; 
the anchor was ready to be dropped, the bowsprit shrouds loose. 
Beside the pilot, who was steering the Pharaon through the narrow entrance to 
the port, there stood a young man, quick of gesture and keen of eye, who 
watched every movement of the ship while repeating each of the pilot's orders.
\end{abstract}

\section{Introduction}

% Move to many cores, why do we need concurrent algorithms/data structures?

In the past decade, advancements in computer performance have been made mostly
through an increasing number of processors instead of higher clock speeds.
This development necessitates new approaches to data structures and algorithms
that take advantage of concurrent execution on multiple threads and processors. 

% The priority queue - semantics and usage summary.

This paper focuses on the priority queue data structure, consisting of two operations
traditionally called \lstinline|Insert| and \lstinline|DeleteMin|. \lstinline|Insert|
places an item into the queue together with its priority, while \lstinline|DeleteMin|
removes and returns the highest priority item. Both of these operations are expected to have
a complexity of at most $O(\log n)$. Priority queues are used in a large variety
of situations such as shortest path algorithms and scheduling. % See liu2012lock for more.

% Concurrent priority queues - heaps, trees, skiplists.

Concurrent priority queues have been the subject of research since the 1980s.
While early efforts have focused mostly on parallelizing Heap structures
\cite{hunt1996efficient}, % TODO: Many more, see \cite{shavit2000skiplist}
more recently priority queues based on \citeauthor{pugh1990skip}'s SkipLists
\cite{pugh1990skip} seem to show more potential \cite{shavit2000skiplist,
sundell2003fast,herlihy2012art,linden2013skiplist}.

% Our plan for the paper - somehow this kind of duplicates the last paragraph though.

In the following, we examine the evolution of concurrent priority queues. We begin
with first steps which used fine-grained locking to avoid the bottleneck of a single
global lock. Initial SkipList-based structures offered better disjoint-access
parallelism, and finally current state of the art lock-free priority queues minimize
contention in \lstinline|DeleteMin|.

% TODO: Mention what we don't cover: non-general PQs, unavailable primitives, ...
% TODO: In section x, we ... Again, this partially duplicates the last paragraph.

\section{Concepts and Definitions}

% Blocking, lock-free and wait-free data structures.

Concurrent data structures are intended to be accessed simultaneously by several processes
at once. \emph{Lock-based} structures ensure that only a limited number of processes may enter
a critical section at once. \emph{Lock-free} data structures eschew the use of locks, and guarantee
that at least a single process makes progress at all times. Since lock-free structures are
non-blocking, they are not susceptible to priority inversion, deadlock, and livelock.
\emph{Wait-freedom} further guarantees that every process finishes each operation in a bounded number of steps. 
In practise, wait-freedom often introduces an unacceptable overhead; lock-freedom
however has proven to be both efficient and to scale well to large numbers of processes.

% Linearizability, sequential and quiescent consistency.

There are several different criteria which allows reasoning about the correctness of concurrent
data structures. \emph{Linearizable} \cite{herlihy1990linearizability} operations appear to take
effect at a single instant in time at so-called linearization points. Linearizability is composable ---
any data structure composed of linearizable objects is also linearizable.
\emph{Sequential consistency} \cite{lamport1979make} requires the result of a set of operations
executed in parallel to be equivalent to the result of some sequential ordering of the same
operations.
\emph{Quiescently consistent} \cite{shavit1996diffracting}
guarantee that the result of a set of parallel operations is equal to the result of a sequential ordering
after a period of quiescence, i.e. an interval without active operations has passed.

% Disjoint-access parallelism.

An area in memory accessed frequently by a large number of processes is said to be \emph{contended}.
Contention is an limiting factor regarding scalability: concurrent reads and writes to the same 
location must be serialized by the cache coherence protocol, and only a single concurrent \ac{CAS} % TODO: Short section on primitives.
can succeed while all others must retry. \emph{Disjoint-access parallelism} is the concept of
spreading such accesses in order to reduce contention as much as possible. 

% Priority queues.

Priority queues are an abstract data structure allowing insertion of items with a given priority
and removal of the highest-priority item in logarithmic time. Search trees and Heaps (which are 
flattened representations of complete trees such that each node's value is at least as large as
those of both children) are concrete data structures which are usually used to implement sequential
priority queues. However, both require fairly elaborate reorganization after \lstinline|DeleteMin|
and/or \lstinline|Insert| operations, which are especially challenging to achieve in a concurrent
environment. 

% SkipLists.

SkipLists \cite{pugh1990skip} have become increasingly popular in concurrent data structures because 
they are both simple and exhibit excellent disjoint-access parallelism. In contrast to
search trees and Heaps, reorganization is not necessary, since SkipLists rely on
randomization for an expected $O(\log n)$ complexity of \lstinline|Insert|, \lstinline|Search|
and \lstinline|Delete| operations.
A SkipList may be visualized as a set of linked lists with corresponding levels. The linked
list at level 0 is the sorted sequence of all objects in the SkipList,
and higher levels provide ``shortcuts''
into the SkipList such that a list of level $i + 1$ contains a subset of the objects in level
$i$. A SkipList node $n$ is said to be of level $i$ if it is in all lists of levels $[0, i]$
and in none of levels $[i + 1, \infty]$. Upon insertion, the new node's level is assigned at random
according to a geometric distribution; deletion simply removes the node.

\section{Fine-grained Locking Heaps}

We chose the priority queue by \citeauthor{hunt1996efficient} \cite{hunt1996efficient}
as a representative of early concurrent priority queues since it has been proven to
perform well \cite{shavit2000skiplist} in comparison to other efforts of the time such as \cite{nageshwara1988concurrent,ayani1990lr,yan1998lock}. % Many others \cite{shavit2000skiplist}.
It is based on a Heap
structure and attempts to minimize lock contention between threads by a) adding per-node
locks, b) spreading subsequent insertions through a bit-reversal technique, % Elaborate on these.
and c) letting insertions traverse bottom-up in order to minimize conflicts with
top-down deletions.

However, significant limitations to scalability remain. A global lock is required
to protect accesses to a variable storing the Heap's size which all operations
must obtain for a short time. Disjoint-access through bit-reversal breaks down
once a certain amount of traffic is reached, since only subsequent insertions
are guaranteed to take disjoint paths towards the root node. Note also that
the root node is a severe serial bottleneck, since it is potentially part of
every insertion path, and necessarily of every \lstinline|DeleteMin| operation.
Finally, in contrast to later SkipList-based designs, the capacity of Hunt Heap
is fixed upon creation.

Benchmarking results in the literature have been mixed; a sequential priority
queue protected by a single global lock outperforms the \citeauthor{hunt1996efficient}
Heap in most cases \cite{hunt1996efficient,sundell2003fast}. Speed-up only occurs once
the size of the Heap reaches a certain threshold such that concurrency
can be properly exploited (instead of being dominated by global locking overhead).

\section{Lock-free Priority Queues}

Traditional data structures such as the Heap have fallen out of favor;
instead, SkipLists \cite{pugh1990skip} have become the focus of modern concurrent
priority queue research
\cite{shavit2000skiplist,sundell2003fast,herlihy2012art,linden2013skiplist,alistarhspraylist}.
SkipLists are both conceptual simple as well as simple to implement; they also exhibit
excellent disjoint-access properties, and do not require rebalancing due to their
reliance on randomization for expected $O(\log n)$ time insertion and deletion bounds.

A state of the art lock-free SkipList implementation based on \ac{CAS}
by \citeauthor{fraser2004practical} \cite{fraser2004practical} is freely available\footnote{
\url{http://www.cl.cam.ac.uk/research/srg/netos/lock-free/}} under a BSD license.
\citeauthor{fraser2004practical} exploits unused pointer bits to mark nodes as logically
deleted, with physical deletion following as a second step.

SkipLists are dynamic data structures in the sense that they grow and shrink
at runtime. In consequence, careful handling of memory accesses and (de)allocations
are required. As an additional requirement, these memory management schemes must
be both scalable and lock-free to avoid limiting the SkipList itself.
\citeauthor{fraser2004practical} in particular employs lock-free epoch-based garbage-collection.

\subsection{Lotan \& Shavit}



\subsection{Sundell \& Tsigas}
\subsection{Linden \& Jonsson}

\section{Relaxed Priority Queues}

\subsection{Wimmer}
\subsection{SprayList}

\begin{comment}
Sections / rough structure:
* Basic concepts and definitions. Linearizability, sequ./quiescent consistency,
  lock-free, wait-free, disjoint-access parallelism (one of the papers has good
  summaries of these).
  Maybe atomic primitives such as CAS (but probably not).
* Priority queue definitions, semantics, usages. Mention inherent
  non-scalability through DeleteMin().
* Skiplist, heap definitions, semantics, usages.
* Follow development of current state of the art from Hunt Heap ~> Shavit ~>
  Tsigas ~> Linden.
* Maybe benchmarks on mars.
\end{comment}

\section{Related Work}

\begin{comment}
  * hunt: most efficient old-school algorithm
  * lots of other older references in [4]
  * israeli, rappoport: wait-free, non-available atomic primitive
  * lotan, shavit [4]: skiplist, lock-based. first to propose usage of skip-lists [11]
  * sundell, tsigas [3]: skiplist, lock-free, linearizable, unique priorities
  * herlihy, shavit in art of multiprocessor programming: based on [4], lock-free.
  * linden, jonsson [11]: skiplist, lock-free, linearizable, reduced mem contention.
    first lock-free PQ algorithm [11, 12]
  * specialized versions such as
    * bounded priorities: [7] and others
    * probabilistic extractMin, extractMany: [9]
\end{comment}

\section{Conclusion}

\begin{comment}
http://leo.stcloudstate.edu/acadwrite/conclude.html
http://writingcenter.unc.edu/handouts/conclusions/
http://www.wikihow.com/Write-a-Conclusion-for-a-Research-Paper
\end{comment}

\printbibliography

\end{document}
